Question #1: Briefly Describe the Technology Innovation.  Up to 500 words
describing the technical innovation that would be the focus of a Phase 1
project, including a brief discussion of the origins of the innovation as well
as an explanation as to why it meets the program’s mandate to focus on
supporting research and development (R&D) of unproven, high-impact innovations.

Stream processing has become ubiquitous in almost every aspect of cloud
computing.  What began as a foundational technology for batch-based and
real-time analytics systems is finding a place in many performance sensitive
integration-based application architectures (iPaaS) and emerging
Internet-of-Things (IoT) applications.  Most stream and event processing is
done using popular Stream Processing Engines (SPE), such as Apache Storm, or
using event-based design patterns within a custom application stack on reliable
event buses, such as Apache Kafka.  While all modern stream processing systems
are distributed systems, these systems are typically deployed in a central
location, such as a virtual private cloud (VPC).

The continued innovation in IoT platforms, such as AWS Greengrass, and increase
in devices connecting to the Internet will also increase the dependence of
business and government processes on IoT.  In addition, the pressure for
automating business and government processes with integration platforms will
only increase as more of the world moves to the SaaS model for productivity,
management and engineering tools.  We posit that relying on the centralized
approach to stream processing is unsuitable for these applications for the
following reasons:

1. As the number of IoT devices and events increase, performing all processing
at a central location (e.g. VPC) may be untenable and will ultimately require
mechanisms for coordinating multiple stream processing systems.  

2. As the number of events increase, so will ingress/egress bandwidth into
sites hosting the SPEs, and as a result, cost will increase and
performance-sensitive applications will suffer.

3. Many stream processing tasks do not require the complexities introduced by
SPEs, but pay the cost of running SPEs to do a bulk of the processing.

The goal of this project is to simplify the placement of stream processing
tasks as a globally distributed system.  The goal is not to usurp existing SPEs
or event based architectures, but to compliment them in a way that performs
processing in the most appropriate tier (at the edge, at a point-of-presence,
on a load balancer or in a centralized SPE) depending on cost, resources and
performance.  For example, a simple transformation, aggregation or filter
operation can occur close to where the event is generated, freeing up resources
in the more expensive SPE for deep learning and sophisticated analytics.

This goal will be accomplished in two ways.  First, we will create a novel
architecture that manages a federation of lightweight, independent processors
that can run directly on commodity hardware, within modern container
orchestration systems, such as Kubernetes, and serverless frameworks, such as
AWS Lambda.  Second, we will provide tools for performing "what-if" analysis,
so researchers and practitioners can experimentally determine optimal placement
of processing under specific constraints.  

Most state-of-the-art improvements in stream processing are focused on running
a distributed stream processing system within a VPC or deploying lightweight
versions of existing stream processing systems to the edge.  Here we are
redefining the stream processing architecture as a globally distributed system
and providing the experimental tools for optimizing placement under a set of
constraints, such as cost, performance and consistency.

Question #2: Briefly Describe the Technical Objectives and Challenges.  Up to
500 words describing the R&D or technical work to be done in a Phase I project,
including a discussion of how and why the proposed work will help prove that
the product or service is technically feasible and/or significantly reduces
technical risk. Discuss how, ultimately, this work could contribute to making
the new product, service, or process commercially viable and impactful. This
section should also convey that the proposed work meets definition of R&D,
rather than straightforward engineering or incremental product development
tasks.

We believe that a successful Phase 1 implementation will mostly be focused on
defining the new architecture, building a reference implementation of the
architecture and building an experimental framework for analyzing the reference
implementation.  The end of Phase 1 will have the following deliverables:

1. An open source implementation of the processor, which will be the kernel of
the new architecture that performs the stream processing tasks.

2. Reference configurations of the new architecture deployed in a public cloud
setting, across a variety of runtimes, such as Linux VMs, Kubernetes
deployments, Kubernetes Jobs and Lambda functions.

3. Empirical comparison of the reference configurations that utilize the new
architecture to that of existing stream processing systems, such as Storm or
Spark, as the core processors.  It is important to determine the efficacy of
the new architecture against the state-of-the-art.  We plan to not only
evaluate performance, durability and cost, but also expected development time.

Most of the technical challenges will come as part of #1 and #3, as reference
configurations are obtained using well-known benchmarks and referencing
real-world use cases.  

We expect to encounter the following technical challenges when defining the new
architecture:

1. Managing stateful processes: Managing distributed stateful operations, such
as aggregations, requires walking the consistency vs. performance curve.  If a
centralized system is used to manage state, we need to account for contention
and round-trip latency.  Distributed aggregation requires some form of
coordination or reconciliation.  Flexibility must be built-in to provide
specific level of consistency, given performance constraints, and vice versa.
This is especially important when determining if processing should occur at the
edge or in a SPE.

2. Durability and consistency: The platform must be able to tolerate crashes
and failures with minimal data loss (ideally none) and never leave the system
in an inconsistent state.  Both of these are table stakes for most stream
processing uses cases.

3. Reliable control plane: The control plane is responsible for managing
processor deployments and configurations, across different networks and targets
(e.g. VMs, containers, stateless, etc.).  We do not know of an existing control
plane that provides these facilities.

The commercial viability of the this project is three-fold:

1. Organizations wishing to deploy IoT processing can use the platform to
manage their or customer edge processing flows.

2. Organizations can leverage the platform for processing events from their or
a customer SaaS-based tools.

3. This can act as a converged stream processing system when leveraging both
IoT and SaaS integrations.

This project aligns with the NSF's R&D requirements for the following reasons:

1. The architecture is novel and revolutionary in that it is moving stream
processing tasks typically performed by cluster-based SPEs to a globally
distributed mesh of processes.  In short, it is a paradigm shift.

2. The project involves a great deal of empirical evidence and experimentation
to prove out the new approach. 

3. The project aims to provide methodologies for further analysis in
globally-distributed stream processing.


Question #3: Briefly Describe the Market Opportunity.  Up to 250 words
describing the customer profile and pain point(s) that will be the near-term
commercial focus related to this technical project.

The global iPaaS market is expected to reach USD 10.3 billion by 2025, largely
driven by challenges with application integration, data transformation and
cloud service orchestration.  This project is aligned with the expected drivers
of iPaaS in the following ways:

1. Processors can be deployed across load balancers to serve as a common data
transformation layer.

2. The cost of performing many stream processing tasks in SPEs can be reduced
by processing in a higher tier, such as load balancers.

3. The project provides a control plane that manages orchestration of the
processors.

The IoT market is expected to grow to USD 1.4 trillion by 2027, with more than
75 billion IoT devices connected to the Internet by 2025.  Two big challenges
with IoT are security and integration management.  This project is aligned with
these challenges in the following ways:

1. Many IoT devices will be running on one or more private networks, which will
ultimately publish events into the organization's private cloud.  In this case,
the control plane can reduce the friction of deploying and managing processes
in the edge and ensure secure connections between the devices (or IoT
gateways), processors and private clouds.

2. The flexibility of the processors will provide those administering the IoT
infrastructure with the tools to perform processing at the appropriate tier.

The platform is expected to shine most in the converged use cases.  That is
organizations leveraging both IoT and iPaaS can use the same tool to manage all
of their stream processing.

Question #4: Briefly Describe the Company and Team?  Up to 250 words describing
the background and current status of the submitting small business, including
team members related to the technical and/or commercial efforts discussed in
this Project Pitch.

The work on this project started in mid-2020 as an open source project.  My
last two jobs required building, leading and working in iPaaS environemnts.  I
noticed that the many companies doing iPaaS were re-inventing the wheel.  That
is, writing the same integration functionality over and over.  In addition, the
integration platforms rely on centralized deployments (e.g. SPEs) to do stream
processing, where a lot of the stream processing tasks did not require the
complexities of a SPE.  Given the similarity, it appears that the world is
moving towards the same patterns for IoT, and will be facing similar problems.
As a result, it seemed obvious that a new approach is needed to solve the next
generation of stream processing problems. 

I am currently working solo on the project and hope to secure funding to
recruit someone else to help and cover the cost of testing our hyptheses in a
cloud environment.  Much of my 15+ year career has been spent working in
distributed systems research (BS/MS/PhD in CS, 20+ peer-reviewed publications
and 8 patents), building production distributed systems and leading teams that
maintain large-scale, customer-facing, distributed systems (3 years at EMC, 6
years at Box and 3 years at other start-ups).  The combination of these
experiences makes me well suited for leading a project of this scope and
sophistication.

